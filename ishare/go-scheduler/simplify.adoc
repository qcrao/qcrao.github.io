= 第 78 期 —— 《Go scheduler 源码阅读》
qcrao - Go 夜读 SIG 小组
v1.0, 2020-03-11
:toc: left
:homepage: https://github.com/developer-learning/reading-go

== 《Go 夜读》介绍

image::https://user-images.githubusercontent.com/7698088/67085429-62d38900-f1d1-11e9-9011-5c5a1f5d08ba.png[width="300", height="200"]

image::https://raw.githubusercontent.com/developer-learning/reading-go/master/static/images/allcontributors-night-reading-go-20191107.jpg[https://allcontributors.org/]

link:https://github.com/developer-learning/night-reading-go/blob/master/HISTORY.md[《Go 夜读》史纪]

== 自我介绍

image::https://user-images.githubusercontent.com/7698088/64483579-222e3a80-d237-11e9-8089-008f89e755f6.png[自我介绍]

# 基础知识

## 汇编基础

应用层代码一般会用到三类 19 个寄存器：

 1. 通用寄存器（64 位）：rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8, r9, r10, r11, r12, r13, r14, r15 寄存器。CPU 对这 16 个通用寄存器的用途没有做特殊规定，程序员和编译器可以自定义其用途（下面会介绍，rsp/rbp 寄存器其实是有特殊用途的）；

 2. 程序计数寄存器（64 位，PC寄存器，有时也叫 IP 寄存器）：rip 寄存器。它用来存放下一条即将执行的指令的地址，这个寄存器决定了程序的执行流程；

 3. 段寄存器：fs 和 gs 寄存器（两个都是 16 位）。一般用它来实现线程本地存储（TLS），比如 AMD64 linux 平台下 go 语言和 pthread 都使用 fs 寄存器来实现系统线程的 TLS。

image::https://user-images.githubusercontent.com/7698088/75600386-b9d75c00-5ae9-11ea-8b57-af4e9242ea86.png[64位 通用寄存器对应的 32/16/8 位寄存器]

`rip 寄存器`：程序计数寄存器。存储的值不是正在被 CPU 执行的指令在内存中的地址，而是紧挨这条正在被执行的指令后面那一条指令的地址。

`rsp 寄存器`：栈顶寄存器。一般用来存放函数调用栈的栈顶地址。

`rbp 寄存器`：栈基址寄存器。通常用来存放函数的栈帧起始地址。

编译器一般使用 `rsp` 和 `rbp` 这两个寄存器加一定偏移的方式来访问函数局部变量或函数参数。

image::https://user-images.githubusercontent.com/7698088/75600683-26a02580-5aed-11ea-95d7-60411ba273d2.png[内存布局图]

 因为不同的 CPU 所支持的机器指令不一样，所以其汇编指令也不同，即使是相同的 CPU，不同的汇编工具和平台所使用的汇编指令格式也有些差别，由于本文主要专注于 AMD64 Linux 平台下的 Go 调度器，因此下面我们只介绍该平台下所使用的 AT&T 格式的汇编指令。

AT&T 汇编指令的基本格式为：

```c
操作码	[操作数]
```

如果操作数有两个，则第一个为源操作数，第二个为目的操作数，目的操作数表示这条指令执行完后结果应该保存的地方。例如：

```c
add   %rdx,%rax
```

上面这条指令表示对 rax 和 rdx 寄存器里面的值求和，并把结果保存在 rax 寄存器中。

对于 AT&T 格式的汇编指令，一些说明如下：

1. 寄存器名需要加 `%` 作为前缀，立即数前加 `$`。
2. 寄存器间接寻址的格式为 `offset(%register)`，如果 `offset` 为 `0`，则可以略去偏移不写直接写成 `(%register)`。
3. 与内存相关的一些指令的操作码会加上 b，w，l 和 q 字母分别表示操作的内存是 1，2，4 还是 8 个字节，比如指令 `movl   $0x0,-0x8(%rbp)` ，操作码 movl 的后缀字母 l 说明我们要把从 `-0x8(%rbp)` 这个地址开始的 4 个内存单元赋值为 0。

`callq` 和 `retq` 指令的执行过程如下：

image::https://user-images.githubusercontent.com/7698088/76159900-fd703c80-615f-11ea-8c12-7eeca7e7a8c7.png[callq & retq]

## Go 汇编基础

NOTE: 需要注意的是，用 Go 汇编语言编写的代码一旦经过汇编器转换成机器指令之后，再用调试工具反汇编出来的代码已经不是 Go 语言汇编代码了，而是跟平台相关的汇编代码。

image::https://user-images.githubusercontent.com/7698088/75603810-084a2200-5b0d-11ea-8c43-a1497b8ec765.png[Go 汇编 和 AMD64 的寄存器名称]

另外，Go 还引入了几个虚拟的（没有对应的硬件）寄存器，目的是方便程序员和编译器用来定位内存中的代码和数据。

```c
// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
TEXT runtime·gogo(SB), NOSPLIT, $16-8
MOVQ buf+0(FP), BX// gobuf -->bx
```

image::https://user-images.githubusercontent.com/7698088/75603913-354b0480-5b0e-11ea-82ca-8484e75e72ff.png[FP 寄存器]

`gogo` 函数通过 `buf+0(FP)` 获取到参数。且由于 BX 等通用寄存器没有区分位数（64、32、16、8），通过操作码来体现，使用后缀：B(8位)、W(16位)、D(32位)或 Q(64位) 来体现。另外，Go 汇编寄存器名字前没有 % 符号。

函数声明中，`NOSPLIT` 指示编译器不要在这个函数中插入检查栈是否溢出的代码。

`$16-8`：数字 16 说明此函数的栈帧大小为 16 字节，8 说明此函数的参数和返回值一共需要占用 8 字节内存（只有一个指针）。

NOTE: Go 函数调用的参数和函数返回值都是放在栈上的，而且这部分栈内存是由调用者而非被调用函数负责预留，所以在函数定义时需要说明到底需要在调用者的栈帧中预留多少空间。

## 系统调用

 系统调用是指使用类似函数调用的方式调用操作系统提供的API。

虽然从概念上来说系统调用和函数调用差不多，但本质上它们有很大的不同，操作系统的代码位于内核地址空间，而 CPU 在执行用户代码时特权等级很低，无权访问需要最高优先级才能访问的内核地址空间的代码和数据，所以不能通过简单的 call 指令直接调用操作系统提供的函数，而需要使用特殊的指令进入操作系统内核完成指定的功能。

另外，用户代码调用操作系统 API 也不是根据函数名直接调用，而是需要根据操作系统为每个 API 提供的一个整型编号来调用，AMD64 Linux 平台约定在进行系统调用时使用 rax 寄存器存放系统调用编号，同时约定使用 rdi, rsi, rdx, r10, r8 和 r9 来传递前 6 个系统调用参数。

## 线程调度

关于操作系统对线程的调度，有两个问题需要搞清楚：

1. 什么时候会发生调度？
2. 调度的时候会做哪些事情？

对于 1，操作系统必须要得到 CPU 的控制权后才能发起调度：

a. 用户程序使用系统调用进入操作系统内核；
b. 硬件中断。硬件中断处理程序由操作系统提供，所以当硬件发生中断时，就会执行操作系统代码。硬件中断有个特别重要的时钟中断，这是操作系统能够发起抢占调度的基础。

操作系统会在执行操作系统代码路径上的某些点检查是否需要调度，所以操作系统对线程的调度也会相应地发生在上述两种情况之下。

对于 2，操作系统会恢复线程的各种寄存器：

操作系统会把不同的线程调度到同一个 CPU 上运行，而每个线程运行时又都会使用 CPU 的寄存器，但每个 CPU 却只有一组寄存器，所以操作系统在把线程 B 调度到 CPU 上运行时需要首先把刚刚正在运行的线程 A 所使用到的寄存器的值全部保存在内存之中，然后再把保存在内存中的线程 B 的寄存器的值全部又放回 CPU 的寄存器，这样线程 B 就能恢复到之前运行的状态接着运行。

恢复 CPU 寄存器的值就相当于改变了 CPU 下一条需要执行的指令，同时也切换了函数调用栈，因此从调度器的角度来说，线程至少包含以下 3 个重要内容：

 1. 一组通用寄存器的值
 2. 将要执行的下一条指令的地址（PC）
 3. 栈（SP、BP）

NOTE: 操作系统对线程的调度可以简单的理解为内核调度器对不同线程所使用的寄存器和栈的切换。

NOTE: 操作系统线程是由内核负责调度且拥有自己私有的一组寄存器值和栈的执行流。

最后提一句：线程本地存储又叫线程局部存储，其英文为 Thread Local Storage，简称 `TLS`，看似一个很高大上的东西，其实就是线程私有的全局变量而已。利用不同线程的 fs 段基址实现。

# Go 调度器

## 概览

Go 程序的执行由两层组成：Go Program，Runtime，即用户程序和运行时。它们之间通过函数调用来实现内存管理、channel 通信、goroutines 创建等功能。用户程序进行的系统调用都会被 Runtime 拦截，以此来帮助它进行调度以及垃圾回收相关的工作。

一个展现了全景式的关系如下图：

image::https://user-images.githubusercontent.com/7698088/62172655-9981cc00-b365-11e9-8912-b16b83930ad0.png[Go runtime]

实际上在操作系统看来，所有的程序都是在执行多线程。将 goroutines 调度到线程上执行，仅仅是 runtime 层面的一个概念，在操作系统之上的层面。

```c
// 程序启动时的初始化代码
......

// 创建 N 个操作系统线程执行 schedule 函数
for i := 0; i < N; i++ {
    create_os_thread(schedule) // 创建一个操作系统线程执行 schedule 函数
}

//schedule函数实现调度逻辑
func schedule() {
   for { //调度循环
         // 根据某种算法从 M 个 goroutine 中找出一个需要运行 的goroutine
         g := find_a_runnable_goroutine_from_M_goroutines()
         
         // CPU 运行该 goroutine，直到需要调度其它 goroutine 才返回
         run_g(g)
         
         // 保存 goroutine 的状态，主要是寄存器的值
         save_status_of_g(g) 
    }
}
```

我们都知道，Go runtime 会负责 goroutine 的生老病死，从创建到销毁，都一手包办。Runtime 会在程序启动的时候，创建 M 个线程（CPU 执行调度的单位），之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型：

image::https://user-images.githubusercontent.com/7698088/61340362-8c001880-a874-11e9-9237-d97e6105cd62.png[M:N scheduling]

在同一时刻，一个线程上只能跑一个 goroutine。当 goroutine 发生阻塞（例如向一个 channel 发送数据，被阻塞）时，runtime 会把当前 goroutine 调度走，让其他 goroutine 来执行。目的就是不让一个线程闲着，榨干 CPU 的每一滴油水。

 所谓的对 goroutine 的调度，是指程序代码按照一定的算法在适当的时候挑选出合适的 goroutine 并放到 CPU 上去运行的过程，这些负责对 goroutine 进行调度的程序代码我们称之为 goroutine 调度器。

有三个基础的结构体来实现 goroutines 的调度：g，m，p。

NOTE: 调度器的职责就是为需要执行的 Go 代码（g）寻找执行者（m）以及执行的准许和资源（p）。

`g` 代表一个 goroutine，它是一个待执行的任务。它包含：表示 goroutine 栈的一些字段，指示当前 goroutine 的状态，指示当前运行到的指令地址，也就是 PC 值。

`m` 表示内核线程，它由操作系统的调度器调度和管理。包含正在运行的 goroutine 等字段。

`p` 代表一个虚拟的 Processor，它可以被看做运行在线程上的本地调度器。它维护一个处于 Runnable 状态的 g 队列，`m` 需要获得 `p` 才能运行 `g`。

还有一个核心的结构体：`sched`，它总览全局。

Runtime 初始化时会启动一些 G：垃圾回收的 G，执行调度的 G，运行用户代码的 G；并且会创建一个 M 用来开始 G 的运行。随着时间的推移，更多的 G 会被创建出来，更多的 M 也会被创建出来。

当然，在 Go 的早期版本，并没有 p 这个结构体，`m` 必须从一个全局的队列里获取要运行的 `g`，因此需要获取一个全局的锁，当并发量大的时候，锁就成了瓶颈。后来在大神 Dmitry Vyokov 的实现里，加上了 `p` 结构体。每个 `p` 自己维护一个处于 Runnable 状态的 `g` 的队列，解决了原来的全局锁问题。

image::https://user-images.githubusercontent.com/7698088/62016513-336e3b00-b1e5-11e9-8923-d5d1743a531b.png[GPM global review]

Go scheduler 的目标：

 For scheduling goroutines onto kernel threads.

image::https://user-images.githubusercontent.com/7698088/61874535-3f26dc80-af1b-11e9-9d9c-127edf90fff9.png[Go scheduler goals]

Go scheduler 的核心思想是：

1. reuse threads；
2. 限制同时运行（不包含阻塞）的线程数为 N，N 等于 CPU 的核心数目；
3. 线程私有的 runqueues，并且可以从其他线程 stealing goroutine 来运行，线程阻塞后，可以将 runqueues 传递给其他线程。

Go scheduler 会启动一个后台线程 sysmon，用来检测长时间（超过 10 ms）运行的 goroutine，将其调度到 global runqueues。这是一个全局的 runqueue，优先级比较低，以示惩罚。

image::https://user-images.githubusercontent.com/7698088/61874781-d55b0280-af1b-11e9-9965-da4efe53d2db.png[Go scheduler limitations]

```c
           L2 ------------+
           |              |
        +--+--+           |
       L1     L1          |
       |       |          |
    +------+------+       |
    | CPU1 | CPU2 |       |
    +------+------+       L3
    | CPU3 | CPU4 |       |
    +------+------+       |
       |       |          |
      L1      L1          |
        +--+--+           |
           |              |
           L2-------------+
```

=== GPM 关系图

image::https://user-images.githubusercontent.com/7698088/62031928-02a8f880-b21b-11e9-96a9-96820452463e.png[GPM relatioship]

=== workflow

image::https://user-images.githubusercontent.com/7698088/62260181-a7a61a00-b443-11e9-849b-b597addeca57.png[goroutine workflow]

=== goroutine 调度时机

image::https://user-images.githubusercontent.com/7698088/76144884-68b60200-60bf-11ea-9eb9-d855c09cde7f.png[调度时机]

=== work stealing

image::https://user-images.githubusercontent.com/7698088/62031928-02a8f880-b21b-11e9-96a9-96820452463e.png[GPM relatioship]

image::https://user-images.githubusercontent.com/7698088/62033338-4ea96c80-b21e-11e9-9167-98767c03d2d9.png[Work Stealing]

=== 同步、异步系统调用

当 G 需要进行系统调用时，根据调用的类型，它所依附的 M 有两种情况：同步、异步。

对于同步的情况，M 会被阻塞，进而从 P 上调度下来，P 可不养闲人，G 仍然依附于 M。之后，一个新的 M 会被调度到 P 上，接着执行 P 的 LRQ 里嗷嗷待哺的 G 们。一旦系统调用完成，G 还会加入到 P 的 LRQ 里，M 则会被“雪藏”，待到需要时再“放”出来。

image::https://user-images.githubusercontent.com/7698088/62091677-b904f000-b2a4-11e9-8972-60ace0807ba4.png[同步系统调用]

对于异步的情况，M 不会被阻塞，G 的异步请求会被“代理人” network poller 接手，G 也会被绑定到 network poller，等到系统调用结束，G 才会重新回到 P 上。M 由于没被阻塞，它因此可以继续执行 LRQ 里的其他 G。

image::https://user-images.githubusercontent.com/7698088/62091486-c2da2380-b2a3-11e9-8cf9-0e63d7f774d8.png[异步系统调用]

可以看到，异步情况下，通过调度，Go scheduler 成功地将 I/O 的任务转变成了 CPU 任务，或者说将内核级别的线程切换转变成了用户级别的 goroutine 切换，大大提高了效率。

 The ability to turn IO/Blocking work into CPU-bound work at the OS level is where we get a big win in leveraging more CPU capacity over time. 

Go scheduler 像一个非常苛刻的监工一样，不会让一个 M 闲着，总是会通过各种办法让你干更多的事。

 In Go, it’s possible to get more work done, over time, because the Go scheduler attempts to use less Threads and do more on each Thread, which helps to reduce load on the OS and the hardware.

=== 调度陷阱示例

由于 Go 语言是协作式的调度，不会像线程那样，在时间片用完后，由 CPU 中断任务强行将其调度走。对于 Go 语言中运行时间过长的 goroutine，Go scheduler 有一个后台线程在持续监控，一旦发现 goroutine 运行超过 10 ms，会设置 goroutine 的“抢占标志位”，之后调度器会处理。但是设置检测位的时机只有在函数“序言”部分，对于没有函数调用的就没有办法了。

> Golang implements a co-operative partially preemptive scheduler. 

所以在某些极端情况下，会掉进一些陷阱：。

```c
func main() {
	var x int
	threads := runtime.GOMAXPROCS(0)
	for i := 0; i < threads; i++ {
		go func() {
			for { x++ }
		}()
	}
	time.Sleep(time.Second)
	fmt.Println("x =", x)
}
```

运行结果是：在死循环里出不来，不会输出最后的那条打印语句。

为什么？上面的例子会启动和机器的 CPU 核心数相等的 goroutine，每个 goroutine 都会执行一个无限循环。

创建完这些 goroutines 后，main 函数里执行一条 `time.Sleep(time.Second)` 语句。Go scheduler 看到这条语句后，简直高兴坏了，要来活了。这是调度的好时机啊，于是主 goroutine 被调度走。先前创建的 `threads` 个 goroutines，刚好“一个萝卜一个坑”，把 M 和 P 都占满了。

在这些 goroutine 内部，又没有调用一些诸如 `channel read block`，`time.sleep` 这些会引发调度器工作的事情。麻烦了，只能任由这些无限循环执行下去了。

解决的办法也有，把 threads 减小 1：

```c
func main() {
	var x int
	threads := runtime.GOMAXPROCS(0) - 1
	for i := 0; i < threads; i++ {
		go func() {
			for { x++ }
		}()
	}
	time.Sleep(time.Second)
	fmt.Println("x =", x)
}
```

运行结果：

```c
x = 0
```

不难理解了吧，主 goroutine 休眠一秒后，被 go schduler 重新唤醒，调度到 M 上继续执行，打印一行语句后，退出。主 goroutine 退出后，其他所有的 goroutine 都必须跟着退出。所谓“覆巢之下 焉有完卵”，一损俱损。

WARNING: 至于为什么最后打印出的 x 为 0，之前的文章link:https://qcrao.com/2019/06/17/cch-says-memory-reorder/[《曹大谈内存重排》]里有讲到过，这里不再深究了。？？？

还有一种解决办法是在 for 循环里加一句：

```c
go func() {
	time.Sleep(time.Second)
	for { x++ }
}()
```

同样可以让 main goroutine 有机会调度执行。

## 源码阅读

== 重要的结构体

文件位置：src/runtime/runtime2.go

=== g
万变不离其宗，系统线程对 goroutine 的调度与内核对系统线程的调度原理是一样的，实质都是通过保存和修改 CPU 寄存器的值来达到切换线程或 goroutine 的目的。

`g` 的结构体，保存了 `goroutine` 的所有信息。调度器代码可以通过 g 对象来对 goroutine 进行调度，当 goroutine 被调离 CPU 时，调度器代码负责把 CPU 寄存器的值保存在 g 对象的成员变量之中，当 goroutine 被调度起来运行时，调度器代码又负责把 g 对象的成员变量所保存的寄存器的值恢复到 CPU 的寄存器。

```c
type g struct {
    ......
}
```


=== p

`p` 结构体用于保存工作线程执行 `go` 代码时所必需的资源，比如 `goroutine` 的运行队列，内存分配用到的缓存等等。

```c
type p struct {
	......
}
```

=== m

每个工作线程在刚刚被创建出来进入调度循环之前就利用线程本地存储机制为该工作线程实现了一个指向 m 结构体实例对象的私有全局变量，这样在之后的代码中就使用该全局变量来访问自己的 m 结构体对象以及与 m 相关联的 p 和 g 对象。

`m` 结构体用来代表工作线程，它保存了 `m` 自身使用的栈信息，当前正在运行的 `goroutine` 以及与 `m` 绑定的 `p` 等信息。

```c
type m struct {
    ......
}
```

=== stack

`stack` 结构体主要用来记录 `goroutine` 所使用的栈的信息，包括栈顶和栈底位置：

```c
// Stack describes a Go execution stack.
// The bounds of the stack are exactly [lo, hi),
// with no implicit data structures on either side.
type stack struct {
	lo uintptr
	hi uintptr
}
```

=== gobuf

`gobuf` 结构体用于保存 `goroutine` 的调度信息，主要包括 `CPU` 的几个寄存器的值：

```c
type gobuf struct {
	sp   uintptr
	pc   uintptr
	g    guintptr
	ctxt unsafe.Pointer
	ret  sys.Uintreg
	lr   uintptr
	bp   uintptr // for GOEXPERIMENT=framepointer
}
```

=== schedt

`schedt` 结构体用来保存调度器的状态信息和 `goroutine` 的全局运行队列。

```c
type schedt struct {
	......
}
```

=== 一些全局变量

```c
var (
	allglen    uintptr
	// 所有的 m 构成的一个链表，包括下面的 m0
	allm       *m
	// 保存所有的 p，len(allp) == gomaxprocs
	allp       []*p  // len(allp) == gomaxprocs; may change at safe points, otherwise immutable
	allpLock   mutex // Protects P-less reads of allp and all writes
	gomaxprocs int32
	// 系统中 cpu 核的数量，程序启动时由 runtime 代码初始化
	ncpu       int32
	forcegc    forcegcstate
	// 调度器结构体对象，记录了调度器的工作状态
	sched      schedt
	newprocs   int32
)
```

== 调度器初始化

任何一个由编译型语言（不管是C，C++，go还是汇编语言）所编写的程序在被操作系统加载起来运行时都会顺序经过如下几个阶段：link:https://mp.weixin.qq.com/s?__biz=MzU1OTg5NDkzOA==&mid=2247483769&idx=1&sn=3d77609a293d87e64639afc8d2219e1c&scene=19#wechat_redirect[出处]

1. 从磁盘上把可执行程序读入内存；

2. 创建进程和主线程；

3. 为主线程分配栈空间；

4. 把由用户在命令行输入的参数拷贝到主线程的栈；

5. 把主线程放入操作系统的运行队列等待被调度执起来运行。

image::https://mmbiz.qpic.cn/mmbiz_png/31W1agpaMjyc0Slcz8bibfMJpvmZLiaeGib0BmHqfrf2XvSISUtQTYLolqSNbHJXx08ppxMicfZkFPrLNIp9aKnwJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1[主线程第一条指令执行前]

程序的入口是：`src/runtime/rt0_linux_amd64.s:8`：

```asm
TEXT _rt0_amd64_linux(SB),NOSPLIT,$-8
	JMP	_rt0_amd64(SB)
```

继续到 `src/runtime/asm_amd64.s:87`，rt0_go 函数完成了 Go 程序启动时的所有初始化工作：

`runtime·rt0_go` 执行完成之后，整个 Go 程序就可以跑起来了，是非常核心的代码。

=== 调整 SP
第一段代码，将 SP 调整到了一个地址是 16 的倍数的位置：

```asm
SUBQ	$(4*8+7), SP		// 2args 2auto
// 调整栈顶寄存器使其按 16 个字节对齐
ANDQ	$~15, SP
```

先是将 SP 减掉 39，也就是向下移动了 39 个 Byte，再进行与运算。

`15` 的二进制低四位是全 1：`1111`，其他位都是 0；取反后，变成了 `0000`，高位则是全 1。这样，与 SP 进行了与运算后，低 4 位变成了全 0，高位则不变。因此 SP 继续向下移动，并且这回是在一个地址值为 16 的倍数的地方，16 字节对齐的地方。

为什么要这么做？画一张图就明白了。不过先得说明一点，前面 `_rt0_amd64_linux` 函数里讲过，DI 里存的是 argc 的值，8 个字节，而 SI 里则存的是 argv 的地址，8 个字节。

image::https://user-images.githubusercontent.com/7698088/64070957-8eda8f80-cca1-11e9-91c7-0b276d7769ea.png[SP 内存对齐]

image::https://user-images.githubusercontent.com/7698088/64070959-a0239c00-cca1-11e9-8ad9-c3aefc5093f8.png[SP 内存对齐]

上面两张图中，左侧用箭头标注了 16 字节对齐的位置。第一步表示向下移动 39 B，第二步表示与 `~15` 相与。

存在两种情况，这也是第一步将 SP 下移的时候，多移了 7 个 Byte 的原因。第一张图里，与 `~15` 相与的时候，SP 值减少了 1，第二张图则减少了 9。最后都是移位到了 16 字节对齐的位置。

两张图的共同点是 SP 与 argc 中间多出了 16 个字节的空位。这个后面应该会用到，我们接着探索。

至于为什么进行 16 个字节对齐，是因为 CPU 有一组 SSE 指令，这些指令中出现的内存地址必须是 16 的倍数。

=== 初始化 g0 栈
接着往后看，开始初始化 g0 的栈了。g0 栈的作用就是为运行 runtime 代码提供一个“环境”。

```asm
// 把 g0 的地址存入 DI
MOVQ	$runtime·g0(SB), DI
// BX = SP - 64*1024 + 104
LEAQ	(-64*1024+104)(SP), BX
// g0.stackguard0 = SP - 64*1024 + 104
MOVQ	BX, g_stackguard0(DI)
// g0.stackguard1 = SP - 64*1024 + 104
MOVQ	BX, g_stackguard1(DI)
// g0.stack.lo = SP - 64*1024 + 104
MOVQ	BX, (g_stack+stack_lo)(DI)
// g0.stack.hi = SP
MOVQ	SP, (g_stack+stack_hi)(DI)
```

代码 L2 把 g0 的地址存入 DI 寄存器；L4 将 SP 下移 (64K-104)B，并将地址存入 BX 寄存器；L6 将 BX 里存储的地址赋给 g0.stackguard0；L8，L10 分别 将 BX 里存储的地址赋给 g0.stackguard1， g0.stack.lo，L12 将 SP 赋值给 g0.stack.hi。

这部分完成之后，g0 栈空间如下图：

image::https://user-images.githubusercontent.com/7698088/64071133-d400c080-cca5-11e9-8563-d5f882e34e0a.png[g0 栈空间]

== 主线程绑定 m0

因为 m0 是全局变量，而 m0 又要绑定到工作线程才能执行。

`runtime` 会启动多个工作线程，每个线程都会绑定一个 m0。代码里还得保持一致，都是用 m0 来表示。

简单来说，TLS（Thread Local Storage）就是线程本地的私有的全局变量。

NOTE: 一般而言，全局变量对进程中的多个线程同时可见。进程中的全局变量与函数内定义的静态（static）变量，是各个线程都可以访问的共享变量。一个线程修改了，其他线程就会“看见”。

NOTE: 如果需要在一个线程内部的各个函数调用都能访问、但其它线程不能访问的变量（被称为 static memory local to a thread，线程局部静态变量），就需要新的机制来实现，这就是 TLS。

NOTE: 只要每个工作线程拥有了各自私有的 m 结构体全局变量，我们就能在不同的工作线程中使用相同的全局变量名来访问不同的 m 结构体对象，这完美的解决我们的问题。

NOTE: 具体到 goroutine 调度器代码，每个工作线程在刚刚被创建出来进入调度循环之前就利用线程本地存储机制为该工作线程实现了一个指向 m 结构体实例对象的私有全局变量，这样在之后的代码中就使用该全局变量来访问自己的 m 结构体对象以及与 m 相关联的 p 和 g 对象。

`tls` 是 m 结构体中的一个数组。

```c
// thread-local storage (for x86 extern register)
tls [6]uintptr
```

设置 tls 的函数 runtime·settls(SB) 位于源码 `src/runtime/sys_linux_amd64.s` 处，主要内容就是通过一个系统调用将 fs 段基址设置成 m.tls[1] 的地址，而 fs 段基址又可以通过 CPU 里的寄存器 fs 来获取。

IMPORTANT: 而每个线程都有自己的一组 CPU 寄存器值，操作系统在把线程调离 CPU 时会帮我们把所有寄存器中的值保存在内存中，调度线程来运行时又会从内存中把这些寄存器的值恢复到 CPU。

IMPORTANT: 工作线程代码就可以通过 fs 寄存器来找到 m.tls。

代码中寄存器前面的符号看着比较奇怪，其实它们最后会被链接器转化为偏移量。

看曹大 golang_notes 用 gobuf_sp(BX) 这个例子讲的：

 这种写法在标准 plan9 汇编中只是个 symbol，没有任何偏移量的意思，但这里却用名字来代替了其偏移量，这是怎么回事呢？

 实际上这是 runtime 的特权，是需要链接器配合完成的，再来看看 gobuf 在 runtime 中的 struct 定义开头部分的注释:

 // The offsets of sp, pc, and g are known to (hard-coded in) libmach.

对于我们而言，这种写法读起来比较容易。

> 从这里还可以看到，保存在主线程本地存储中的值是 g0 的地址，也就是说工作线程的私有全局变量其实是一个指向 g 的指针而不是指向 m 的指针。

> 目前这个指针指向 g0，表示代码正运行在 g0 栈。

于是，前面的图又增加了新的玩伴 m0：

image::https://user-images.githubusercontent.com/7698088/75735730-54c47600-5d36-11ea-912a-7ab8dcbda1dc.png[工作线程绑定 m0，g0]

=== 初始化 m0

```c
// src/runtime/proc.go:532
// The bootstrap sequence is:
//
//	call osinit
//	call schedinit
//	make & queue new G
//	call runtime·mstart
//
// The new G calls runtime·main.
func schedinit() {
	......
}
```

这个函数开头的注释很贴心地把 Go 程序初始化的过程又说了一遍：

1. call osinit。初始化系统核心数。
2. call schedinit。初始化调度器。
3. make & queue new G。创建新的 goroutine。
4. call runtime·mstart。调用 mstart，启动调度。
5. The new G calls runtime·main。在新的 goroutine 上运行 runtime.main 函数。

函数首先调用 `getg()` 函数获取当前正在运行的 `g`，`getg()` 在 `src/runtime/stubs.go` 中声明，真正的代码由编译器生成。

```c
// getg returns the pointer to the current g.
// The compiler rewrites calls to this function into instructions
// that fetch the g directly (from TLS or from the dedicated register).
func getg() *g
```

`getg` 返回当前正在运行的 goroutine 的指针，它会从 tls 里取出 tls[0]，也就是当前运行的 goroutine 的地址。编译器插入类似下面的代码：

```c
get_tls(CX) 
MOVQ g(CX), BX; // BX存器里面现在放的是当前g结构体对象的地址
```

将 m 挂到全局变量 allm 上，allm 是一个指向 m 的的指针。

```c
mp.alllink = allm
atomicstorep(unsafe.Pointer(&allm), unsafe.Pointer(mp))
```

第二行将 allm 变成 m 的地址，这样变成了一个循环链表。之后再新建 m 的时候，新 m 的 alllink 就会指向本次的 m，最后 allm 又会指向新创建的 m。

image::https://user-images.githubusercontent.com/7698088/63501720-bcd00f00-c4fe-11e9-9642-1757de67aaa1.png[m.alllink 形成链表]

上图中，1 将 m0 挂在 allm 上。之后，若新创建 m，则 m1 会和 m0 相连。

完成这些操作后，大功告成！解锁。

 从这个函数的源代码可以看出，`mcommoninit` 并未对 m0 做什么关于调度相关的初始化，所以可以简单的认为这个函数只是把 m0 放入全局链表 allm 之中就返回了。

=== 初始化 allp

回到 `schedinit()` 函数里来，跳过一些其他的初始化代码，继续往后看：

```c
// src/runtime/proc.go
    procs := ncpu
	if n, ok := atoi32(gogetenv("GOMAXPROCS")); ok && n > 0 {
		procs = n
	}
	if procresize(procs) != nil {
		throw("unknown runnable goroutine during bootstrap")
	}
```

这里就是设置 procs，它决定创建 P 的数量。ncpu 这里已经被赋上了系统的核心数，因此代码里不设置 GOMAXPROCS 也是没问题的。如果环境变量设置了，就使用环境变量设置的值。

考虑到初始化完成之后用户代码还可以通过 GOMAXPROCS() 函数调用它重新创建和初始化 p 结构体对象，而在运行过程中再动态的调整 p 牵涉到的问题比较多，所以这个函数的处理比较复杂，但如果只考虑初始化，相对来说要简单很多，所以这里只看初始化时会执行的代码。

```c
// src/runtime/proc.go:4160
func procresize(nprocs int32) *p {
   ......
}
```

代码比较长，这个函数不仅是初始化的时候会执行到，在中途改变 procs 的值的时候，仍然会调用它。所有存在很多一般不用关心的代码，因为一般不会在中途重新设置 procs 的值。

函数先是从堆上创建了 nproc 个 P，并且把 P 的状态设置为 `_Pgcstop`，现在全局变量 allp 里就维护了所有的 P。

接着，调用函数 `acquirep` 将 p0 和 m0 关联起来。我们来详细看一下：

```c
func acquirep(_p_ *p) {
	......
}

func wirep(_p_ *p) {
	......
}
```

可以看到就是一些字段相互设置，执行完成后：

```c
g0.m.p = p0
p0.m = m0
```

并且，p0 的状态变成了 `_Prunning`。

函数 `runqempty` 用来判断一个 P 是否是空闲，依据是 P 的本地 run queue 队列里有没有 runnable 的 G，如果没有，那 P 就是空闲的。

```c
// src/runtime/proc.go

// Defend against a race where 1) _p_ has G1 in runqnext but runqhead == runqtail,
// 2) runqput on _p_ kicks G1 to the runq, 3) runqget on _p_ empties runqnext.
// Simply observing that runqhead == runqtail and then observing that runqnext == nil
// does not mean the queue is empty.

// 如果 _p_ 的本地队列里没有待运行的 G，则返回 true
func runqempty(_p_ *p) bool {
// 这里涉及到一些数据竞争，并不是简单地判断 runqhead == runqtail 并且 runqnext == nil 就可以
//
for {
	head := atomic.Load(&_p_.runqhead)
	tail := atomic.Load(&_p_.runqtail)
	runnext := atomic.Loaduintptr((*uintptr)(unsafe.Pointer(&_p_.runnext)))
	if tail == atomic.Load(&_p_.runqtail) {
		return head == tail && runnext == 0
	}
}
}
```

并不是简单地判断 head == tail 并且 runnext == nil 为真，就可以说明 runq 是空的。因为涉及到一些数据竞争，例如在比较 head == tail 时为真，但此时 runnext 上其实有一个 G，之后再去比较 runnext == nil 的时候，这个 G 又通过 runqput跑到了 runq 里去了或者通过 runqget 拿走了，runnext 也为真，于是函数就判断这个 P 是空闲的，这就会形成误判。

因此 runqempty 函数先是通过原子操作取出了 head，tail，runnext，然后再次确认 tail 没有发生变化，最后再比较 head == tail 以及 runnext == nil，保证了在观察三者都是在“同时”观察到的，因此，返回的结果就是正确的。

NOTE: 读 head 和 tail 的那一时刻两者是相等的。

说明一下，runnext 上有时会绑定一个 G，这个 G 是被当前 G 唤醒的，相比其他 G 有更高的执行优先级，因此把它单独拿出来。 

函数的最后，初始化了一个“随机分配器”：

```c
stealOrder.reset(uint32(nprocs))
```

将来有些 m 去偷工作的时候，会遍历所有的 P，这时为了偷地随机一些，就会用到 stealOrder 来返回一个随机选择的 P，后面的文章会再讲。

这样，整个 procresize 函数就讲完了，这也意味着，调度器的初始化工作已经完成了。总结一下：

> 1. 使用 make([]*p, nprocs) 初始化全局变量 allp，即 allp = make([]*p, nprocs)
> 2. 循环创建并初始化 nprocs 个 p 结构体对象并依次保存在 allp 切片之中
> 3. 把 m0 和 allp[0] 绑定在一起，即 m0.p = allp[0]，allp[0].m = m0
> 4. 把除了 allp[0] 之外的所有 p 放入到全局变量 sched 的 pidle 空闲队列之中

最后一步，代码里是将所有空闲的 P 放入到调度器的全局空闲队列；对于非空闲的 P（本地队列里有 G 待执行），则是生成一个 P 链表，返回给 procresize 函数的调用者。

最后我们将 allp 和 allm 都添加到图上：

image::https://user-images.githubusercontent.com/7698088/64071128-97cd6000-cca5-11e9-95a9-344f2a0a6474.png[g0-p0-m0]

== 创建 main goroutine

上一节我们讲完了 Go scheduler 的初始化，现在调度器一切就绪，就差被调度的实体了。本节就来讲述 main goroutine 是如何诞生，并且被调度的。

`schedinit` 完成调度系统初始化后，返回到 rt0_go 函数中开始调用 newproc() 创建一个新的 goroutine 用于执行 mainPC 所对应的 runtime·main 函数，看下面的代码：

继续看代码，前面我们完成了 `schedinit` 函数，这是 runtime·rt0_go 函数里的一步，接着往后看：

```asm
# create a new goroutine to start program
# 创建一个新的 goroutine 来启动程序
MOVQ	$runtime·mainPC(SB), AX		# entry

......
```

代码前面几行是在为调用 newproc 函数构造栈，执行完 `runtime·newproc(SB)` 后，就会以一个新的 goroutine 来执行 mainPC 也就是 `runtime.main()` 函数。`runtime.main()` 函数最终会执行到我们写的 main 函数，舞台交给我们。

```c
// src/runtime/proc.go
// 创建一个新的 g，运行 fn 函数，需要 siz byte 的参数
// 将其放至 G 队列等待运行
// 编译器会将 go 关键字的语句转化成此函数

//go:nosplit
func newproc(siz int32, fn *funcval)
```

当我们随手一行代码：

```c
go func() {
    // 要做的事
}()
```

在 Go 编译器的作用下，这条语句最终会转化成 newproc 函数。

`newproc` 函数需要两个参数：一个是新创建的 goroutine 需要执行的任务，也就是 fn，它代表一个函数 func；还有一个是 fn 的参数大小。

再回过头看，构造 newproc 函数调用栈的时候，第一个参数是 0，因为 runtime.main 函数没有参数：

```c
// src/runtime/proc.go

func main()
```

第二个参数则是 runtime.main 函数的地址。

可能会感到奇怪，为什么要给 `newproc` 传一个表示 fn 的参数大小的参数呢？

我们知道，goroutine 和线程一样，都有自己的栈，不同的是 goroutine 的初始栈比较小，只有 2K，而且是可伸缩的，这也是创建 goroutine 的代价比创建线程代价小的原因。

NOTE: 每个 goroutine 都有自己的栈空间，newproc 函数会新创建一个新的 goroutine 来执行 fn 函数，在新 goroutine 上执行指令，就要用新 goroutine 的栈。

NOTE: 执行函数需要参数，这个参数又是在老的 goroutine 上，所以需要将其拷贝到新 goroutine 的栈上。拷贝的起始位置就是栈顶，那拷贝多少数据呢？由 siz 来确定。

`newproc` 函数的第二个参数：

```c
type funcval struct {
	fn uintptr
	// variable-size, fn-specific data here
}
```

它是一个变长结构，第一个字段是一个指针 fn，内存中，紧挨着 fn 的是函数的参数。

link:https://changkun.de/golang/zh-cn/part2runtime/ch06sched/stack/[欧神《Go 语言原本》的一个例子]

```c
// src/runtime/proc.go:3376
func newproc(siz int32, fn *funcval) {
    // 函数调用参数入栈顺序是从右向左，而且栈是从高地址向低地址增长的
    // 注意：argp 指向 fn 函数的第一个参数，而不是 newproc 函数的参数
    // 参数 fn 在栈上的地址 +8 的位置存放的是 fn 函数的第一个参数
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	// 获取正在运行的 g，初始化时是 m0.g0
	gp := getg()
	// getcallerpc() 返回一个地址，也就是调用 newproc 时由 call 指令压栈的函数返回地址，
    // 对于我们现在这个场景来说，pc 就是 CALLruntime·newproc(SB) 指令后面的 POPQ AX 这条指令的地址
	pc := getcallerpc
	// systemstack 的作用是切换到 g0 栈执行作为参数的函数
    // 我们这个场景现在本身就在 g0 栈，因此什么也不做，直接调用作为参数的函数
	systemstack(func() {
		newproc1(fn, argp, siz, gp, pc)
	})
}
```

`newproc1` 函数的第一个参数 `fn` 是新创建的 goroutine 需要执行的函数，注意这个 `fn` 的类型是 `funcval` 结构体类型。

`newproc1` 的第二个参数 argp 是 fn 函数的第一个参数的地址，第三个参数是 fn 函数的参数以字节为单位的大小，后面两个参数我们不用关心。这里需要注意的是，newproc1 是在 g0 的栈上执行的。

```c
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) {
    ......
}
```

尝试从 p0 上找一个空闲的 G：

```c
// 从 p 的本地缓冲里获取一个没有使用的 g，初始化时为空，返回 nil
newg := gfget(_p_)
```

如果拿不到，则会在堆上创建一个新的 G，为其分配 2KB 大小的栈，并设置好新 goroutine 的 stack 成员，设置其状态为 _Gdead，并将其添加到全局变量 allgs 中。创建完成之后，我们就在堆上有了一个 2K 大小的栈。于是，我们的图再次丰富：

image::https://user-images.githubusercontent.com/7698088/64071207-1ecf0800-cca7-11e9-874f-a907e272581c.png[创建了新的 goroutine]

这样，main goroutine 就诞生了。

上一讲讲完了 main goroutine 的诞生，它不是第一个，算上 g0，它要算第二个了。不过，我们要考虑的就是这个 goroutine，它会真正执行用户代码。

`g0` 栈用于执行调度器的代码，执行完之后，要跳转到执行用户代码的地方，如何跳转？这中间涉及到栈和寄存器的切换。

函数调用和返回主要靠的也是 CPU 寄存器的切换。`goroutine` 的切换和此类似。

继续看 `proc1` 函数的代码。中间有一段调整运行空间的代码，计算出的结果一般为 0，也就是一般不会调整 SP 的位置，忽略好了。


```c
if narg > 0 {
    // 把参数从执行 newproc 函数的栈（初始化时是 g0 栈）拷贝到新 g 的栈
	memmove(unsafe.Pointer(spArg), argp, uintptr(narg))
	......
}
```


将 fn 的参数从 g0 栈上拷贝到 newg 的栈上，memmove 函数需要传入源地址、目的地址、参数大小。由于 main 函数在这里没有参数需要拷贝，因此这里相当于没做什么。

接着，初始化 newg 的各种字段，而且涉及到最重要的 pc，sp 等字段：

```c
// 把 newg.sched 结构体成员的所有成员设置为 0
memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
// 设置 newg 的 sched 成员，调度器需要依靠这些字段才能把 goroutine 调度到 CPU 上运行
newg.sched.sp = sp
newg.stktopsp = sp
// newg.sched.pc 表示当 newg 被调度起来运行时从这个地址开始执行指令
newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
newg.sched.g = guintptr(unsafe.Pointer(newg))
gostartcallfn(&newg.sched, fn)
newg.gopc = callerpc
newg.ancestors = saveAncestors(callergp)
// 设置 newg 的 startpc 为 fn.fn，该成员主要用于函数调用栈的 traceback 和栈收缩
// newg 真正从哪里开始执行并不依赖于这个成员，而是 sched.pc
newg.startpc = fn.fn
if _g_.m.curg != nil {
	newg.labels = _g_.m.curg.labels
}
```

`memclrNoHeapPointers` 将 newg.sched 的内存全部清零。接着，设置 sched 的 sp 字段，当 goroutine 被调度到 m 上运行时，需要通过 sp 字段来指示栈顶的位置，这里设置的就是新栈的栈顶位置。

最关键的一行来了：

```c
// newg.sched.pc 表示当 newg 被调度起来运行时从这个地址开始执行指令
newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
```

设置 `pc` 字段为函数 `goexit` 的地址加 1，也说是 `goexit` 函数的第二条指令，`goexit` 函数是 `goroutine` 退出后的一些清理工作。有点奇怪，这是要干嘛？接着往后看。

```c
newg.sched.g = guintptr(unsafe.Pointer(newg))
```

设置 `g` 字段为 newg 的地址。插一句，sched 是 g 结构体的一个字段，它本身也是一个结构体，保存调度信息。复习一下：

```c
type gobuf struct {
	// 存储 rsp 寄存器的值
	sp   uintptr
	// 存储 rip 寄存器的值
	pc   uintptr
	// 指向 goroutine
	g    guintptr
	ctxt unsafe.Pointer // this has to be a pointer so that gc scans it
	// 保存系统调用的返回值
	ret  sys.Uintreg
	lr   uintptr
	bp   uintptr // for GOEXPERIMENT=framepointer
}
```

接下来的这个函数非常重要，可以解释之前为什么要那样设置 `pc` 字段的值。调用 `gostartcallfn`：

```c
gostartcallfn(&newg.sched, fn) //调整 sched 成员和 newg 的栈
```

传入 newg.sched 和 fn。

```c
func gostartcallfn(gobuf *gobuf, fv *funcval) {
	......
}
```


函数 `gostartcallfn` 只是拆解出了包含在 funcval 结构体里的函数指针，转过头就调用 `gostartcall`。将 sp 减小了一个指针的位置，这是给返回地址留空间。果然接着就把 buf.pc 填入了栈顶的位置：

```c
*(*uintptr)(unsafe.Pointer(sp)) = buf.pc
```

原来 buf.pc 只是做了一个搬运工。重新设置 buf.sp 为减掉一个指针位置之后的值，设置 buf.pc 为 fn，指向要执行的函数，这里就是指的 runtime.main 函数。

之后，当调度器“光顾”此 goroutine 时，取出 buf.sp 和 buf.pc，恢复 CPU 相应的寄存器，就可以构造出 goroutine 的运行环境。

而 goexit 函数也通过“偷天换日”将自己的地址“强行”放到 newg 的栈顶，达到自己不可告人的目的：

每个 goroutine 执行完之后，都要经过我的一些清理工作，才能“放行”。这样一说，goexit 函数还真是无私，默默地做一些“扫尾”的工作。

设置完 newg.sched 之后，我们的图又可以前进一步：

image::https://user-images.githubusercontent.com/7698088/64071278-73738280-cca9-11e9-9a67-2570ceea3724.png[设置 newg.sched]

上图中，newg 新增了 sched.pc 指向 `runtime.main` 函数，当它被调度起来执行时，就从这里开始；新增了 sched.sp 指向了 newg 栈顶位置，同时，newg 栈顶位置的内容是一个跳转地址，指向 `runtime.goexit` 的第二条指令，当 goroutine 退出时，这条地址会载入 CPU 的 PC 寄存器，跳转到这里执行“扫尾”工作。

`newg` 的状态变成可执行后（Runnable），就可以将它加入到 P 的本地运行队列里，等待调度。所以，goroutine 何时被执行，用户代码决定不了。来看源码：

```c

// 将 G 放入 _p_ 的本地待运行队列
// 初始化的时候一定是 p 的本地运行队列，其它时候可能因为本地队列满了而放入全局队列
runqput(_p_, newg, true)

// runqput 尝试将 g 放到本地可执行队列里。
// 如果 next 为假，runqput 将 g 添加到可运行队列的尾部
// 如果 next 为真，runqput 将 g 添加到 p.runnext 字段
// 如果 run queue 满了，runnext 将 g 放到全局队列里
//
// runnext 成员中的 goroutine 会被优先调度起来运行
func runqput(_p_ *p, gp *g, next bool) {
	......
}
```

`runqput` 函数的主要作用就是将新创建的 goroutine 加入到 P 的可运行队列，如果本地队列满了，则加入到全局可运行队列。

前两个参数都好理解，最后一个参数 `next` 的作用是，当它为 true 时，会将 newg 加入到 P 的 runnext 字段，具有最高优先级，将先于普通队列中的 goroutine 得到执行。

先将 P 老的 runnext 成员取出，接着用一个原子操作 cas 来试图将 runnext 成员设置成 newg，目的是防止其他线程在同时修改 runnext 字段。

设置成功之后，相当于 newg “挤掉” 了原来老的处于 runnext 的 goroutine，还得给人遣散费，安顿好人家嘛，不然和强盗有何区别？

“安顿”的动作在 retry 代码段中执行。先通过 `head`，`tail`，`len(_p_.runq)` 来判断队列是否已满，如果没满，则直接写到队列尾部，同时修改队列尾部的指针。

如果本地队列满了，那就只能试图将 newg 添加到全局可运行队列中了。调用 `runqputslow(_p_, gp, h, t)` 完成。

```c
// 将 g 和 _p_ 本地队列的一半 goroutine 放入全局队列。
// 因为要获取锁，所以会慢
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
	......
}
```

先将 P 本地队列里所有的 goroutine 加入到一个数组中，数组长度为 `len(_p_.runq)/2 + 1`，也就是 runq 的一半加上 newg。

接着，将从 runq 的头部开始的前一半 goroutine 存入 bacth 数组。然后，使用原子操作尝试修改 P 的队列头，因为出队了一半 goroutine，所以 head 要向后移动 1/2 的长度。

如果修改失败，说明 runq 的本地队列被其他线程修改了，因此后面的操作就不进行了，直接返回 false，表示 newg 没被添加进来。

通过循环将 batch 数组里的所有 g 串成链表：

```c
for i := uint32(0); i < n; i++ {
	batch[i].schedlink.set(batch[i+1])
}
```

image::https://user-images.githubusercontent.com/7698088/63630942-09c4fa00-c653-11e9-8919-dc6b8eb957f1.png[批量 goroutine 连接成链表]

最后，将链表添加到全局队列中。由于操作的是全局队列，因此需要获取锁，因为存在竞争，所以代价较高。这也是本地可运行队列存在的原因。

调用 `globrunqputbatch(&q, int32(n+1))`：

```c
// Put a batch of runnable goroutines on the global runnable queue.
// This clears *batch.
// Sched must be locked.
func globrunqputbatch(batch *gQueue, n int32) {
	sched.runq.pushBackAll(*batch)
	sched.runqsize += n
	*batch = gQueue{}
}
```

如果全局的队列尾 `q.tail` 不为空，则直接将其和前面生成的链表头相接，否则说明全局的可运行列队为空，那就直接将前面生成的链表头设置到 sched.runqhead。

最后，再设置好队列尾，增加 runqsize。

设置完成之后：

image::https://user-images.githubusercontent.com/7698088/63630946-0f224480-c653-11e9-9f97-ce12db645399.png[放到全局可运行队列]

再回到 `runqput` 函数，如果将 newg 添加到全局队列失败了，说明本地队列在此过程中发生了变化，又有了位置可以添加 newg，因此重试 retry 代码段。我们也可以发现，P 的本地可运行队列的长度为 256，它是一个循环队列，因此最多只能放下 256 个 goroutine。

因为此时处于初始化的场景，所以 newg 被成功放入 p0 的本地可运行队列，等待被调度。

将我们的图再完善一下：

image::https://user-images.githubusercontent.com/7698088/64071321-699e4f00-ccaa-11e9-9ef0-b18bafcb7806.png[newg 添加到本地 runq]

 1. 首先，main goroutine 对应的 newg 结构体对象的 sched 成员已经完成了初始化，图中只显示了 pc 和 sp 成员，pc 成员指向了 runtime.main 函数的第一条指令，sp 成员指向了 newg 的栈顶内存单元，该内存单元保存了 runtime.main 函数执行完成之后的返回地址，也就是 runtime.goexit 函数的第二条指令，预期 runtime.main 函数执行完返回之后就会去执行 runtime.exit 函数的 CALL runtime.goexit1(SB) 这条指令；
 2. 其次，newg 已经放入与当前主线程绑定的 p 结构体对象的本地运行队列，因为它是第一个真正意义上的 goroutine，还没有其它 goroutine，所以它被放在了本地运行队列的头部；
 3. 最后，newg 的 m 成员为 nil，因为它还没有被调度起来运行，也就没有跟任何 m 进行绑定。

== 开始调度循环

前面创建了一个 goroutine，设置好了 sched 成员的 sp 和 pc 字段，并且将其添加到了 p0 的本地可运行队列，坐等调度器的调度。

继续看代码。搞了半天，我们其实还在 `runtime·rt0_go` 函数里，执行完 `runtime·newproc(SB)` 后，两条 POP 指令将之前为调用它构建的参数弹出栈。好消息是，最后就只剩下一个函数了：

```c
// start this M
// 主线程进入调度循环，运行刚刚创建的 goroutine
CALL	runtime·mstart(SB)
```

这到达了本系列的核心区，前面铺垫了半天，调度器终于要开始运转了。

```c
// src/runtime/proc.go:1041

func mstart() {
	_g_ := getg()

    ......
	_g_.stackguard0 = _g_.stack.lo + _StackGuard
	_g_.stackguard1 = _g_.stackguard0
	mstart1()

	.......
}
```

`mstart` 函数设置了 stackguard0 和 stackguard1 字段后，就直接调用 mstart1() 函数：

```c
// src/runtime/proc.go:1075

func mstart1() {
    ......
}
```

`mstart1` 首先调用 save 函数来保存 g0 的调度信息，save 这一行代码非常重要，是我们理解调度循环的关键点之一。

这里首先需要注意的是代码中的 `getcallerpc()` 返回的是 mstart 调用 mstart1 时被 call 指令压栈的返回地址，`getcallersp()` 函数返回的是调用 mstart1 函数之前 mstart 函数的栈顶地址，其次需要看看 save 函数到底做了哪些重要工作。

```c
func save(pc, sp uintptr) {
	_g_ := getg()

	_g_.sched.pc = pc // 再次运行时的指令地址
	_g_.sched.sp = sp // 再次运行时的栈顶
	_g_.sched.lr = 0
	_g_.sched.ret = 0
	_g_.sched.g = guintptr(unsafe.Pointer(_g_))
	// We need to ensure ctxt is zero, but can't have a write
	// barrier here. However, it should always already be zero.
	// Assert that.
	if _g_.sched.ctxt != nil {
		badctxt()
	}
}
```

image::https://user-images.githubusercontent.com/7698088/76136279-9f156200-606a-11ea-9edb-2bd63a0276a9.png[调用 save 函数后]

注：图中 sched.pc 并不直接指向返回地址，所以图中的虚线并没有箭头。

 从上图可以看出，g0.sched.sp 指向了 mstart1 函数执行完成后的返回地址，该地址保存在了 mstart 函数的栈帧之中；g0.sched.pc 指向的是 mstart 函数中调用 mstart1 函数之后的 switch 语句。

WARNING: 为什么 g0 已经执行到 mstart1 这个函数了而且还会继续调用其它函数，但 g0 的调度信息中的 pc 和 sp 却要设置在 mstart 函数中？难道下次切换到 g0 时要从 mstart 函数中的 switch 语句继续执行？可是从 mstart 函数可以看到，switch 语句之后就要退出线程了！

`save` 函数执行完成后，返回到 mstart1 继续其它跟 m 相关的一些初始化，完成这些初始化后则调用调度系统的核心函数 schedule() 完成 goroutine 的调度，之所以说它是核心，原因在于每次调度 goroutine 都是从 schedule 函数开始的。

```c
// src/runtime/proc.go:2446

// 执行一轮调度器的工作：找到一个 runnable 的 goroutine，并且执行它
// 永不返回
func schedule() {
    ......
}
```

调用 `runqget`，从 P 本地可运行队列先选出一个可运行的 goroutine；为了公平，调度器每调度 61 次的时候，都会尝试从全局队列里取出待运行的 goroutine 来运行，调用 `globrunqget`；

如果还没找到，就要去其他 P 里面去偷一些 goroutine 来执行，调用 `findrunnable` 函数。

经过千辛万苦，终于找到了可以运行的 goroutine，调用 `execute(gp, inheritTime)` 切换到选出的 goroutine 栈执行，调度器的调度次数会在这里更新。

```c
// src/runtime/proc.go:2040
func execute(gp *g, inheritTime bool) {
    ......
}
```

execute 函数的第一个参数 gp 即是需要调度起来运行的 goroutine，这里首先把 gp 和 m 关联起来，然后把 gp 的状态从 _Grunnable 修改为 _Grunning，这样通过 m 就可以找到当前工作线程正在执行哪个 goroutine，反之亦然。

IMPORTANT: gogo 函数也是通过汇编语言编写的，这里之所以需要使用汇编，是因为 goroutine 的调度涉及不同执行流之间的切换，前面我们在讨论操作系统切换线程时已经看到过，执行流的切换从本质上来说就是 CPU 寄存器以及函数调用栈的切换，然而不管是 go 还是 C 这种高级语言都无法精确控制 CPU 寄存器的修改，因而高级语言在这里也就无能为力了，只能依靠汇编指令来达成目的。

```c
// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
src/runtime/asm_amd64.s:273
TEXT runtime·gogo(SB), NOSPLIT, $16-8
    ......
```

最精彩的时刻：

```asm
MOVQ   gobuf_pc(BX), BX
```

把 gp.sched.pc 的值读取到 BX 寄存器，这个 pc 值是 gp 这个 goroutine 马上需要执行的第一条指令的地址，对于我们这个场景来说它现在就是 runtime.main 函数的第一条指令，现在这条指令的地址就放在 BX 寄存器里面。最后一条指令：

```asm
JMP BX
```

这里的 `JMP BX` 指令把 `BX` 寄存器里面的指令地址放入 `CPU` 的 `rip` 寄存器，于是，CPU 就会跳转到该地址继续执行属于 gp 这个 goroutine 的代码，这样就完成了 goroutine 的切换。

 总结一下这15条指令，其实就只做了两件事：
 1. 把 gp.sched 的成员恢复到 CPU 的寄存器完成状态以及栈的切换；
 2. 跳转到 gp.sched.pc 所指的指令地址（runtime.main）处执行。

现在已经从 g0 切换到了 gp 这个 goroutine，对于我们这个场景来说，gp 还是第一次被调度起来运行，它的入口函数是 runtime.main，所以接下来 CPU 就开始执行 runtime.main 函数：

```c
// src/runtime/proc.go:113
func main() {
	......
}
```

从上述流程可以看出，runtime.main 执行完 main 包的 main 函数之后就直接调用 exit 系统调用结束进程了，它并没有返回到调用它的函数（还记得是从哪里开始执行的 runtime.main 吗？），

其实 runtime.main 是 main goroutine 的入口函数，并不是直接被调用的，而是在 `schedule()->execute()->gogo()` 这个调用链的 gogo 函数中用汇编代码直接跳转过来的，所以从这个角度来说，goroutine 确实不应该返回，没有地方可返回啊！

可是从前面的分析中我们得知，在创建 goroutine 的时候已经在其栈上放好了一个返回地址，伪造成 goexit 函数调用了 goroutine 的入口函数，这里其实是为非 main goroutine 准备的，非 main goroutine 执行完成后就会返回到 goexit 继续执行，而 main goroutine 执行完成后整个进程就结束了，这是 main goroutine 与其它 goroutine 的一个区别。

用一张流程图总结一下从 g0 切换到 main goroutine 的过程：

image::https://user-images.githubusercontent.com/7698088/63644111-b6ff4700-c713-11e9-8961-664ec101030a.png[从 g0 到 gp]

非 main goroutine 的退出会调用 `goexit` 函数：

```asm
TEXT runtime·goexit(SB),NOSPLIT,$0-0
	BYTE	$0x90	// NOP
	CALL	runtime·goexit1(SB)	// does not return
	// traceback from goexit1 must hit code range of goexit
	BYTE	$0x90	// NOP
```

从前面的分析我们已经看到，非 main goroutine 返回时直接返回到了 goexit 的第二条指令：

```asm
CALL	runtime·goexit1(SB)
```

该指令继续调用 goexit1 函数：

```asm
// Finishes execution of the current goroutine.
func goexit1() {
	if raceenabled {
		racegoend() // 与竞态检查有关，不关注
	}
	if trace.enabled {
		traceGoEnd()
	}
	mcall(goexit0) // 与 backtrace 有关，不关注
}
```

`goexit1` 函数通过调用 mcall 从当前运行的 g2 goroutine 切换到 g0，然后在 g0 栈上调用和执行 goexit0 这个函数。

```asm
// func mcall(fn func(*g))
// Switch to m->g0's stack, call fn(g).
// Fn must never return. It should gogo(&g->sched)
// to keep running g.
# mcall的参数是一个指向 funcval 对象的指针
# 主要作用就是保存当前 goroutine 的现场，然后切换到 g0 栈去调用作为参数传递给它的函数
TEXT runtime·mcall(SB), NOSPLIT, $0-8
    ......
```

mcall 函数主要有两个功能：

1. 首先从当前运行的 g 切换到 g0，这一步包括保存当前 g 的调度信息，把 g0 设置到 tls 中，修改 CPU 的 rsp 寄存器使其指向 g0 的栈；

2. 以当前运行的 g 为参数调用 fn 函数(此处为 goexit0)。

```c
// goexit continuation on g0.
// src/runtime/proc.go:2809
func goexit0(gp *g) {
	......
}
```

`goexit0` 函数完成最后的清理工作：

1. 把 g 的状态从 _Grunning 变更为 _Gdead；
2. 把 g 的一些字段清空成 0 值；
3. 调用 dropg 函数解除 g 和 m 之间的关系，其实就是设置 `g->m = nil, m->currg = nil`；
4. 把 g 放入 p 的 freeg 队列缓存起来供下次创建 g 时快速获取而不用从内存分配。freeg 就是 g 的一个对象池；
5. 调用 schedule 函数再次进行调度。

到此为止一个普通的 g 的生命周期就结束了，工作线程再次调用了 schedule 函数进入新一轮的调度循环。

总结一下 main goroutine 和普通 goroutine 的退出过程：

对于 main goroutine，在执行完用户定义的 main 函数的所有代码后，直接调用 exit(0) 退出整个进程，非常霸道。

对于普通 goroutine，先是跳转到提前设置好的 goexit 函数的第二条指令，然后调用 runtime.goexit1，接着调用 `mcall(goexit0)`，而 mcall 函数会切换到 g0 栈，运行 goexit0 函数，清理 goroutine 的一些字段，并将其添加到 goroutine 缓存池里，然后进入 schedule 调度循环。到这里，普通 goroutine 才算完成使命。

image::https://user-images.githubusercontent.com/7698088/76141225-2380d880-609d-11ea-91a7-75f06cb3c4a9.png[调度循环]

如图所示，rt0_go 负责 Go 程序启动的所有初始化，中间进行了很多初始化工作，调用 mstart 之前，已经切换到了 g0 栈，图中不同色块表示使用不同的栈空间。

接着调用 gogo 函数，完成从 g0 栈到用户 goroutine 栈的切换，包括 main goroutine 和普通 goroutine。

之后，执行 main 函数或者用户自定义的 goroutine 任务。

执行完成后，main goroutine 直接调用 eixt(0) 退出，普通 goroutine 则调用 goexit -> goexit1 -> mcall，完成普通 goroutine 退出后的清理工作，然后切换到 g0 栈，调用 goexit0 函数，将普通 goroutine 添加到缓存池中，再调用 schedule 函数进行新一轮的调度。

```c
schedule() -> execute() -> gogo() -> goroutine 任务 -> goexit() -> goexit1() -> mcall() -> goexit0() -> schedule()
```

1. 可以看出，一轮调度从调用 schedule 函数开始，经过一系列过程再次调用 schedule 函数来进行新一轮的调度，从一轮调度到新一轮调度的过程称之为一个调度循环。

2. 这里说的调度循环是指某一个工作线程的调度循环，而同一个 Go 程序中存在多个工作线程，每个工作线程都在进行着自己的调度循环。

3. 从前面的代码分析可以得知，上面调度循环中的每一个函数调用都没有返回，虽然 `goroutine 任务-> goexit() -> goexit1() -> mcall()` 是在 g 的栈空间执行的，但剩下的函数都是在 g0 的栈空间执行的。

4. 每次执行 mcall 切换到 g0 栈时都是切换到 g0.sched.sp 所指的固定位置，这之所以行得通，正是因为从 schedule 函数开始之后的一系列函数永远都不会返回，所以重用这些函数上一轮调度时所使用过的栈内存是没有问题的。

5. 栈空间在调用函数时会自动“增大”，而函数返回时，会自动“减小”，这里的增大和减小是指栈顶指针 SP 的变化。上述这些函数都没有返回，说明调用者不需要用到被调用者的返回值。

6. 因为 g0 一直没有动过，所有它之前保存的 sp 还能继续使用。每一次调度循环都会覆盖上一次调度循环的栈数据，完美！

## GPM 状态机
先从最简单的 M 看起：

image::https://user-images.githubusercontent.com/7698088/64058333-09d97280-cbdc-11e9-8a4d-1843d5be88d0.png[M 的状态流转图, width="500"]

M 只有自旋和非自旋两种状态。自旋的时候，会努力找工作；找不到的时候会进入非自旋状态，之后会休眠，直到有工作需要处理时，被其他工作线程唤醒，又进入自旋状态。

再来看 P，P 的数量一般不会发生变化，有多少个逻辑核心，就有多少个 P：

image::https://user-images.githubusercontent.com/7698088/64058164-93d40c00-cbd9-11e9-9095-7bc7248a0fb9.png[P 的状态流转图, width="500"]

 通常情况下（在程序运行时不调整 P 的个数），P 只会在上图中的四种状态下进行切换。 当程序刚开始运行进行初始化时，所有的 P 都处于 `_Pgcstop` 状态， 随着 P 的初始化（`runtime.procresize`），会被置于 `_Pidle`。

 当 M 需要运行时，会 `runtime.acquirep` 来使 P 变成 `Prunning` 状态，并通过 `runtime.releasep` 来释放。 

 当 G 执行时需要进入系统调用，P 会被设置为 `_Psyscall`， 如果这个时候被系统监控抢夺（`runtime.retake`），则 P 会被重新修改为 `_Pidle`。 

 如果在程序运行中发生 `GC`，则 P 会被设置为 `_Pgcstop`， 并在 `runtime.startTheWorld` 时重新调整为 `_Prunning`。

最后是 G：

image::https://user-images.githubusercontent.com/7698088/64057782-d98dd600-cbd3-11e9-918d-8320fd9609c0.png[G 的状态流转图, width="500"]


# 参考资料
【阿波张】https://mp.weixin.qq.com/mp/homepage?__biz=MzU1OTg5NDkzOA==&hid=1&sn=8fc2b63f53559bc0cee292ce629c4788&scene=18#wechat_redirect

【xiaorui.cc】http://xiaorui.cc/archives/6535

【GMP 原理】https://mp.weixin.qq.com/s/4gMdGH4ssgeYwQi34mEzhg

【欧神 启动流程】https://changkun.de/golang/zh-cn/part1basic/ch05life/boot/

【知乎回答，怎样理解阻塞非阻塞与同步异步的区别】https://www.zhihu.com/question/19732473/answer/241673170

【从零开始学架构 Reactor与Proactor】https://book.douban.com/subject/30335935/

【思否上 goalng 排名第二的大佬译文】https://segmentfault.com/a/1190000016038785

【ardan labs】https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html

【论文 Analysis of the Go runtime scheduler】http://www.cs.columbia.edu/~aho/cs6998/reports/12-12-11_DeshpandeSponslerWeiss_GO.pdf

【译文传播很广的】https://morsmachine.dk/go-scheduler

【码农翻身文章】https://mp.weixin.qq.com/s/BV25ngvWgbO3_yMK7eHhew

【goroutine 资料合集】https://github.com/ardanlabs/gotraining/tree/master/topics/go/concurrency/goroutines

【大彬调度器系列文章】http://lessisbetter.site/2019/03/10/golang-scheduler-1-history/

【Scalable scheduler design doc 2012】https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit#heading=h.rvfa6uqbq68u

【Go scheduler blog post】https://morsmachine.dk/go-scheduler

【work stealing】https://rakyll.org/scheduler/

【Tony Bai 也谈goroutine调度器】https://tonybai.com/2017/06/23/an-intro-about-goroutine-scheduler/

【Tony Bai  调试实例分析】https://tonybai.com/2017/11/23/the-simple-analysis-of-goroutine-schedule-examples/

【Tony Bai goroutine 是如何工作的】https://tonybai.com/2014/11/15/how-goroutines-work/

【How Goroutines Work】https://blog.nindalf.com/posts/how-goroutines-work/

【知乎回答 什么是阻塞，非阻塞，同步，异步？】https://www.zhihu.com/question/26393784/answer/328707302

【知乎文章 完全理解同步/异步与阻塞/非阻塞】https://zhuanlan.zhihu.com/p/22707398

【The Go netpoller】https://morsmachine.dk/netpoller

【知乎专栏 Head First of Golang Scheduler】https://zhuanlan.zhihu.com/p/42057783

【鸟窝 五种 IO 模型】https://colobu.com/2019/07/26/IO-models/

【Go Runtime Scheduler】https://speakerdeck.com/retervision/go-runtime-scheduler?slide=32

【go-scheduler】https://povilasv.me/go-scheduler/#

【追踪 scheduler】https://www.ardanlabs.com/blog/2015/02/scheduler-tracing-in-go.html

【go tool trace 使用】https://making.pusher.com/go-tool-trace/

【goroutine 之旅】https://medium.com/@riteeksrivastava/a-complete-journey-with-goroutines-8472630c7f5c

【介绍 concurreny 和 parallelism 区别的视频】https://www.youtube.com/watch?v=cN_DpYBzKso&t=422s

【scheduler 的陷阱】http://www.sarathlakshman.com/2016/06/15/pitfall-of-golang-scheduler

【boya 源码阅读】https://github.com/zboya/golang_runtime_reading/blob/master/src/runtime/proc.go

【阿波张调度器系列教程】http://mp.weixin.qq.com/mp/homepage?__biz=MzU1OTg5NDkzOA==&hid=1&sn=8fc2b63f53559bc0cee292ce629c4788&scene=18#wechat_redirect

【曹大 asmshare】https://github.com/cch123/asmshare/blob/master/layout.md

【Go调度器介绍和容易忽视的问题】https://www.cnblogs.com/CodeWithTxT/p/11370215.html

【最近发现的一位大佬的源码分析】https://github.com/changkun/go-under-the-hood/blob/master/book/zh-cn/TOC.md